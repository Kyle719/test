# -*- coding: utf-8 -*-
"""ì‹¤ìŠµ_7_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kq2lkngLvgw2VGsxzozpRKGVFQs5bUz5

# ê°ì„±ë¶„ì„ ì‹¤ìŠµ

<b>í•™ìŠµ ëª©í‘œ:    
- í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ì˜ ì „ë°˜ì ì¸ FLOWë¥¼ ì´í•´í•œë‹¤.
- keras.Sequantial ëª¨ë“ˆì„ ì´ìš©í•´ ê°„ë‹¨í•œ ê°ì„±ë¶„ì„ ëª¨ë¸ì„ êµ¬í˜„í•´ í•™ìŠµí•˜ê³ , í•™ìŠµ ê²°ê³¼ë¥¼ ì§„ë‹¨í•œë‹¤.</b>

<font color = "red"> 
QUIZ:   
ìˆ«ìë§Œ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ê¸°ê³„í•™ìŠµ ëª¨ë¸ì—ê²Œ ìì—°ì–´ë¥¼ ì¸ì‹ì‹œí‚¤ëŠ” ë°©ë²•ì€? </font>
"""

# Commented out IPython magic to ensure Python compatibility.
try: 
#   %tensorflow_version 2.x
except Exception:
  pass

import numpy as np

import tensorflow as tf

from tensorflow.keras import Model

from tensorflow.keras import layers
import tensorflow_datasets as tfds
tfds.disable_progress_bar()

""" í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ """
!pip install konlpy

"""# # 1. ìì—°ì–´ì²˜ë¦¬ í”Œë¡œìš° ì´í•´í•˜ê¸°

### Step 1. Parsing
- konply : í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ ê´€ë ¨ íŒ¨í‚¤ì§€
- konplyì˜ Okt taggerì„ ì´ìš©í•´ í˜•íƒœì†Œ ë¶„ì„ ì‹¤í–‰

<img src = "https://github.com/seungyounglim/temporary/blob/master/image_2.PNG?raw=true">
"""

from konlpy.tag import Okt
okt=Okt()

def tokenize(lines): 
  return [pos[0] for pos in okt.pos(lines)]

sentence1 = "ì‹œê°„ ê°€ëŠ” ì¤„ ì•Œê³  ë´¤ìŠµë‹ˆë‹¤."
sentence2 = "ì•ˆë³´ë©´ í›„íšŒã… ã… ..."
parsed_sent1 = tokenize(sentence1)
parsed_sent2 = tokenize(sentence2)
print("ë¬¸ì¥ 1:", parsed_sent1)
print("ë¬¸ì¥ 2:", parsed_sent2)

"""### Step 2. ëª¨ë¸ ì¸í’‹ ë§Œë“¤ê¸°

<img src = "https://github.com/seungyounglim/temporary/blob/master/image_3.PNG?raw=true">

#### 2-1) ë‹¨ì–´ ì‚¬ì „ ë§Œë“¤ê¸°
ìì—°ì–´ í˜•íƒœì†Œë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•´ì•¼ í•¨
- í˜•íƒœì†Œ ë¶„ì„ëœ ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ ë§¤í•‘í•˜ëŠ” ì‚¬ì „ ë§Œë“¤ê¸°
- ë°°ì¹˜ ì—°ì‚°ì„ ìœ„í•´ í•„ìš”í•œ Padding([PAD])ê³¼ Out of vocabulary([OOV]) í† í°ì„ í•­ìƒ ë§¨ ì•ì— ì¶”ê°€í•´ë‘ 
"""

vocab_dict = {}
vocab_dict["[PAD]"] = 0
vocab_dict["[OOV]"] = 1
i = 2
for word in parsed_sent1:
    if word not in vocab_dict.keys():
        vocab_dict[word] = i
        i += 1
for word in parsed_sent2:
    if word not in vocab_dict.keys():
        vocab_dict[word] = i
        i += 1
print("Vocab Dictionary Example:")
print(vocab_dict)

"""#### 2-2) vocab_dictë¥¼ ì´ìš©í•´ ìì—°ì–´ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°
- ìœ„ì—ì„œ ë§Œë“  vocab_dictë¥¼ ì´ìš©í•´ íŒŒì‹±í•´ë‘” ë¬¸ì¥ì„ ëª¨ë¸ì— íƒœìš¸ ìˆ˜ ìˆëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°
- ê¸°ë³¸ì ìœ¼ë¡œ LSTMì€ ê°€ë³€ì ì¸ ë¬¸ì¥ ê¸¸ì´ë¥¼ ì¸í’‹ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆì§€ë§Œ, ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ <font color="blue">max_seq_len</font>ì„ ì •í•´ë‘ê³  ê¸¸ì´ë¥¼ í†µì¼í•¨    
    - max_seq_len ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ì—ëŠ” max_seq_lenì´ ë  ë•Œê¹Œì§€ [PAD]ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë¶™ì—¬ì¤Œ
    - max_seq_len ë³´ë‹¤ ê¸´ ë¬¸ì¥ì€ max_seq_len ê°œì˜ í† í°ë§Œ ë‚¨ê¸°ê³  ìë¦„   
       - ì•ì—ì„œë¶€í„° max_seq_len ë§Œí¼ì˜ í† í°ë§Œ ì‚¬ìš©í•œë‹¤ê±°ë‚˜
       - ë’¤ì—ì„œë¶€í„° max_seq_len ë§Œí¼ì˜ í† í°ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜
       - ì¤‘ê°„ë¶€ë¶„ì—ì„œ max_seq_len ë§Œí¼ë§Œ ì‚¬ìš©í•¨
    - tensorflow.keras.preprocessing.sequenceì˜ <font color="blue">pad_sequences</font> ì‚¬ìš©
"""

max_seq_len = 10

input_id1 = [vocab_dict[word] for word in parsed_sent1]
input_id2 = [vocab_dict[word] for word in parsed_sent2]

# Padding
from tensorflow.keras.preprocessing.sequence import pad_sequences
input_ids = [input_id1, input_id2]
input_ids = pad_sequences(input_ids, maxlen=max_seq_len, value = vocab_dict['[PAD]']) 
print(input_ids)

"""### Step3. ëª¨ë¸ ë§Œë“¤ê¸°

<img src = "https://github.com/seungyounglim/temporary/blob/master/image_4.PNG?raw=true">

- <b>tf.keras.Sequential()</b> ì„ ì‚¬ìš©í•´ ëª¨ë¸ êµ¬í˜„í•˜ê¸°
- Sequential()ì€ ë ˆì´ì–´ë¥¼ ì—°ì†ì ìœ¼ë¡œ ìŒ“ì•„ì„œ ëª¨ë¸ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ
    - ì„ë² ë”© ë ˆì´ì–´ : layers.Embedding()
    - LSTM : layers.LSTM()
    - FC layer : layers.Dense()   
- LSTMì„ ì‚¬ìš©í•´ ë¬¸ì¥ì„ ì¸ì½”ë”©í•˜ê³ , Fully Connected layerì„ ë‘ ì¸µ ìŒ“ì•„ ìµœì¢… outputì„ ìƒì„±
"""

vocab_size = len(vocab_dict) # ë‹¨ì–´ì‚¬ì „ ê°œìˆ˜
embedding_dim = 30 # ì„ë² ë”© ì°¨ì›
lstm_hidden_dim = 50 # LSTM hidden_size 
dense_dim = 50 #FC layer size
batch_size = 2 # batch size

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim),
    tf.keras.layers.LSTM(lstm_hidden_dim),
    tf.keras.layers.Dense(dense_dim, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])

"""- <b>model.summary()</b> : ëª¨ë¸ êµ¬ì¡°, íŒŒë¼ë©”í„° ê°œìˆ˜ë¥¼ í•œ ëˆˆì— ë³´ì—¬ì¤Œ"""

model.summary()

"""- <b>tf.keras.utils.plot_model()</b> : ì¸í’‹ ~ ì•„ì›ƒí’‹ê¹Œì§€ í…ì„œì˜ íë¦„ì„ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ëƒ„"""

tf.keras.utils.plot_model(model, "LSTM_sentiment_analysis.png", show_shapes = True)

"""- <b>model.predict()</b> ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì¸í’‹ì— ëŒ€í•´ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŒ."""

scores = model.predict(input_ids)

for i, s in enumerate(scores):
    print("ë¬¸ì¥ {} â†’ ê¸ì •: {:.2f} / ë¶€ì •: {:.2f}".format(i, s[0],s[1]))

"""# # 2. LSTMìœ¼ë¡œ ê°ì„±ë¶„ì„ ëª¨ë¸ í›ˆë ¨í•˜ê¸°

### Step 0. í•™ìŠµ ë°ì´í„° ì¤€ë¹„í•˜ê¸°
<img src = "https://github.com/seungyounglim/temporary/blob/master/image_5.PNG?raw=true">    

- ë„¤ì´ë²„ ì˜í™” ê°ì„±ë¶„ì„ ë°ì´í„°ì…‹ í™œìš©
- í›ˆë ¨ ë°ì´í„° 150,000ê±´, í…ŒìŠ¤íŠ¸ ë°ì´í„° 50,000ê±´
"""

""" ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ """
!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt
!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt

""" ë°ì´í„° ì½ì–´ì˜¤ê¸° """

with open("ratings_train.txt") as f:
    raw_train = f.readlines()
with open("ratings_test.txt") as f:
    raw_test = f.readlines()
raw_train = [t.split('\t') for t in raw_train[1:]]
raw_test = [t.split('\t') for t in raw_test[1:]]

FULL_TRAIN = []
for line in raw_train:
    FULL_TRAIN.append([line[0], line[1], int(line[2].strip())])
FULL_TEST = []
for line in raw_test:
    FULL_TEST.append([line[0], line[1], int(line[2].strip())])
print("FULL_TRAIN: {}ê°œ (ê¸ì • {}, ë¶€ì • {})".format(len(FULL_TRAIN), sum([t[2] for t in FULL_TRAIN]), len(FULL_TRAIN)-sum([t[2] for t in FULL_TRAIN])), FULL_TRAIN[0])
print("FULL_TEST : {}ê°œ (ê¸ì • {}, ë¶€ì • {})".format(len(FULL_TEST), sum([t[2] for t in FULL_TEST]), len(FULL_TEST)-sum([t[2] for t in FULL_TEST])), FULL_TEST[0])

"""### label 
> 0: ë¶€ì •

> 1: ê¸ì •
"""

# ë°ì´í„° ì˜ˆì‹œ : id, ë¬¸ì¥, ë¼ë²¨ ìˆœì„œ
print(FULL_TRAIN[0])

"""<img src = "https://github.com/seungyounglim/temporary/blob/master/image_6.PNG?raw=true">  
- ì‹œê°„ ê´€ê³„ìƒ train ì¤‘ 50,000ê±´ì„ í•™ìŠµë°ì´í„°, 10,000ê±´ì„ ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©
- test ì¤‘ 10,000ê±´ë§Œ ìƒ˜í”Œë§í•˜ì—¬ ìµœì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©
"""

import random
random.seed(1)
random.shuffle(FULL_TRAIN)
random.shuffle(FULL_TEST)
train = FULL_TRAIN[:50000]
val = FULL_TRAIN[50000:60000]
test = FULL_TEST[:10000]
print("train     : {}ê°œ (ê¸ì • {}, ë¶€ì • {})".format(len(train), sum([t[2] for t in train]), len(train)-sum([t[2] for t in train])), train[0])
print("validation: {}ê°œ (ê¸ì • {}, ë¶€ì • {})".format(len(val), sum([t[2] for t in val]), len(val)-sum([t[2] for t in val])), val[0])
print("test      : {}ê°œ (ê¸ì • {}, ë¶€ì • {})".format(len(test), sum([t[2] for t in test]), len(test)-sum([t[2] for t in test])), test[0])

"""## Step 1. Parsing
- Train/ Testì˜ ë¬¸ì¥ì„ í˜•íƒœì†Œë¶„ì„ê¸°ë¡œ íŒŒì‹±í•˜ì—¬ train_sentences, test_sentencesì— ì €ì¥í•´ë‘ .
- categorical_crossentropy lossë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì •ë‹µ ë¼ë²¨ì€ One-hot encoding í˜•ì‹ìœ¼ë¡œ ì €ì¥
   - ë¶€ì • -> [1, 0]
   - ê¸ì • -> [0 , 1]
"""

train_sentences = []
val_sentences = []
test_sentences = []

# ì¶”í›„ í•™ìŠµ/ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë¼ë²¨ ì •ë³´ ì €ì¥í•´ë‘ 
train_label_ids = []
val_label_ids = []
test_label_ids = []

print("start tokenizing TRAIN sentences")
for i, line in enumerate(train):
    tokens = tokenize(line[1])
    train_sentences.append(tokens)
    if line[2] == 0: # ë¶€ì •
      train_label_ids.append([1,0])
    else: #ê¸ì •
      train_label_ids.append([0,1])

    if (i+1) % 5000 == 0: print("... {}/{} done".format(i+1, len(train)))

print("example:", train_sentences[-1], train_label_ids[-1], "\n")

print("start tokenizing VALIDATION sentences")

for line in val:
    tokens = tokenize(line[1])
    val_sentences.append(tokens)
    if line[2] == 0: # ë¶€ì •
      val_label_ids.append([1,0])
    else: #ê¸ì •
      val_label_ids.append([0,1])
print("... done\n")

print("start tokenizing TEST sentences")
for line in test:
    tokens = tokenize(line[1])
    test_sentences.append(tokens)
    if line[2] == 0: # ë¶€ì •
      test_label_ids.append([1,0])
    else: #ê¸ì •
      test_label_ids.append([0,1])

print("... done")

"""##Step 2. ëª¨ë¸ ì¸í’‹ ë§Œë“¤ê¸°

#### 2-1) ë‹¨ì–´ì‚¬ì „ ë§Œë“¤ê¸°
- í›ˆë ¨ ë°ì´í„° ë¬¸ì¥ì— ìˆëŠ” í˜•íƒœì†Œë¥¼ ì´ìš©í•´ êµ¬ì¶•
- (ì¼ë°˜ì ìœ¼ë¡œëŠ” ë” ë§ì€ ì½”í¼ìŠ¤ì— ëŒ€í•´ êµ¬ì¶•ëœ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì§€ë§Œ, í¸ì˜ìƒ í›ˆë ¨ì…‹ë§Œìœ¼ë¡œ ì§„í–‰)

# ì‹¤ìŠµ MISSION #14
[CODE] ë¶€ë¶„ì„ ì±„ì›Œë„£ì–´ ë‹¨ì–´ì‚¬ì „ì„ ë§Œë“¤ê³  ìƒì„±ëœ ë‹¨ì–´ì‚¬ì „ì˜ í¬ê¸°ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.
"""

from tqdm import tqdm

vocab_dict = {}
vocab_dict["[PAD]"] = 0
vocab_dict["[OOV]"] = 1
i = 2
for sentence in train_sentences:
    for word in sentence:
        if word not in vocab_dict.keys():
            ## [CODE] ##
            vocab_dict[word] = i
            ############
            i += 1
print("Vocab Dictionary Size:", len(vocab_dict))

"""#### 2-2) vocab_dictë¥¼ ì´ìš©í•´ ìì—°ì–´ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°

# ì‹¤ìŠµ MISSION #15
> í† í°í™”ëœ ë¬¸ì¥ë“¤ (tokenized_sentences)ì„ ì¸í’‹ìœ¼ë¡œ ë°›ì•„ ë‹¤ìŒì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“œì‹œì˜¤

* ë‹¨ì–´ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ëŠ” [OOV] ì¸ë±ìŠ¤ë¡œ ì²˜ë¦¬í•˜ê¸°   
* ë‹¨ì–´ì‚¬ì „ì—ì„œ ë§¤í•‘ë˜ëŠ” ë‹¨ì–´ëŠ” í•´ë‹¹ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°   
* ë¬¸ì¥ ê¸¸ì´ë¥¼ 'max_seq_len'ìœ¼ë¡œ ë§ì¶”ì–´, max_seq_lenë³´ë‹¤ ê¸´ ë¬¸ì¥ì€ ë’·ë¶€ë¶„ì„ ìë¥´ê³ , max_seq_lenë³´ë‹¤ ì§§ì€ ë¬¸ì¥ì€ ë’·ë¶€ë¶„ì— paddingí•˜ê¸°
"""

def make_input_ids(tokenized_sentences, max_seq_len = 50):
  
  num_oov = 0 # OOV ë°œìƒ ê°œìˆ˜ë¥¼ ì…ˆ
  result_input_ids = [] # result_input_ids : ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•œ ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸

  for sentence in tokenized_sentences :
      """ vocab_dictë¥¼ ì‚¬ìš©í•´ ì •ìˆ˜ë¡œ ë³€í™˜ """ 
      input_ids = []
      for token in sentence:
          if token not in vocab_dict: 
              input_ids.append(vocab_dict['[OOV]']) ## a. [CODE] OOV ì²˜ë¦¬
              num_oov += 1
          else:
              input_ids.append(vocab_dict[token]) ## b. [CODE] ë‹¨ì–´ì‚¬ì „ì—ì„œ í† í° ì°¾ì•„ì„œ ë¶™ì´ê¸°
      
      result_input_ids.append(input_ids)
      
  """ max_seq_lenì„ ë„˜ëŠ” ë¬¸ì¥ì€ ì ˆë‹¨, ëª¨ìë¥´ëŠ” ê²ƒì€ PADDING """
  result_input_ids = pad_sequences(result_input_ids, maxlen=max_seq_len, padding='post', truncating='post', value = 0) ## c. [CODE] padding í•˜ê¸°

  return result_input_ids, num_oov

# train_sentences ì²˜ë¦¬
train_input_ids, num_oov = make_input_ids(train_sentences)

print("---- TRAIN ----")
print("... # OOVs     :", num_oov)

# val_sentences ì²˜ë¦¬
val_input_ids, num_oov = make_input_ids(val_sentences)

print("---- VALIDATION ----")
print("... # OOVs     :", num_oov)

# test_sentences ì²˜ë¦¬
test_input_ids, num_oov = make_input_ids(test_sentences)

print("---- TEST ----")
print("... # OOVs     :", num_oov)

"""#### 2-3) ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ë¥¼ np.arrayë¡œ ë³€í™˜
- TIP: tensorflow2.0ì—ì„œëŠ” numpy arrayë¥¼ ì¸í’‹ìœ¼ë¡œ ë°›ì•„ë“¤ì„
"""

train_label_ids = np.array(train_label_ids)
val_label_ids = np.array(val_label_ids)
test_label_ids = np.array(test_label_ids)

"""## Step3. ëª¨ë¸ ë§Œë“¤ê¸°

# ì‹¤ìŠµ MISSION #16
> ì•„ë˜ ì¡°ê±´ì— ë§ëŠ” ëª¨ë¸ì„ ë§Œë“œì‹œì˜¤
 
* embedding ì°¨ì›ì€ 150
* LSTM hidden sizeëŠ” 100
* Denseì˜ hidden sizeëŠ” 100, relu activation ì‚¬ìš©
* output Dense layerì—ì„œëŠ” ê¸/ë¶€ì • 2ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•˜ë˜ softmax ì‚¬ìš©
"""

tf.keras.backend.clear_session()

from tensorflow.keras.layers import Embedding, LSTM, Dense

vocab_size = len(vocab_dict) 

model = tf.keras.Sequential([
            ####### MISSION ì‘ì„± ######
            tf.keras.layers.Embedding(vocab_size, 150),
            tf.keras.layers.Dropout(rate = 0.2),
            tf.keras.layers.LSTM(100),
            tf.keras.layers.Dropout(rate = 0.2),
            tf.keras.layers.Dense(100, activation='relu'),
            tf.keras.layers.Dense(2, activation='softmax')
            ###########################
])

model.summary()

"""## Step 4. ëª¨ë¸ í›ˆë ¨í•˜ê¸°

#### 4-1) <b>model.compile()</b>ì„ í†µí•´ loss, optimizer ì§€ì •
"""

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

"""#### 4-2) model.fit()ì„ í†µí•´ ëª¨ë¸ í›ˆë ¨"""

num_epochs = 5
history = model.fit(train_input_ids, train_label_ids, epochs=num_epochs, validation_data=(val_input_ids, val_label_ids), verbose=2) 

test_result = model.evaluate(test_input_ids, test_label_ids, verbose=2)

"""<font color='purple'>ğŸš´â€â™€ï¸<i> while training...</i></font>   
<br>
<u> keras RNN API í™•ì¸í•˜ê¸°</u>
- https://www.tensorflow.org/guide/keras/rnn
- ê¸°ë³¸ì ì¸ RNN ì´ì™¸ì— Bidirectional RNN, Multi-layer RNN êµ¬ì¡° ë“±ì„ í™œìš©í•˜ê³  ì‹¶ë‹¤ë©´ API ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ë§Œë“¤ ìˆ˜ ìˆìŒ.
- ì˜ˆ) 
  - LSTMì˜ ëª¨ë“  timestepì˜ outputì„ ë°›ì•„ì˜¤ë ¤ë©´ 
  - lstm = tf.keras.layers.LSTM(hidden_dim, return_sequences=True)ë¡œ ì„¤ì •
  - Bidirectional-LSTMì„ ì‚¬ìš©í•˜ë ¤ë©´
  - layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10))

#### 4-3) í›ˆë ¨ ê²°ê³¼ ì§„ë‹¨í•˜ê¸°
<font color="red">QUIZ :   
a. í˜„ì¬ ëª¨ë¸ì— ë¬¸ì œì ì´ ìˆë‚˜ìš”?   
b. ë¬¸ì œê°€ ë‚˜íƒ€ë‚˜ê³  ìˆë‹¤ë©´ ì´ì— ëŒ€í•œ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•´ ë³´ì„¸ìš”. 
</font>
"""

import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

"""## Step 5. Inference ì‹¤í–‰í•˜ê¸°"""

""" í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë‹¤ì‹œ ì˜ˆì¸¡í•´ë³´ê¸° """

def inference(mymodel, sentence):
  # 1. tokenizerë¡œ ë¬¸ì¥ íŒŒì‹±
  parsed_sent = tokenize(sentence)
  input_id = []

  # 2. vocab_dictë¥¼ ì´ìš©í•´ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
  for word in parsed_sent:
    if word in vocab_dict: input_id.append(vocab_dict[word])
    else: input_id.append(vocab_dict["[OOV]"])
  
  # ë‹¨ì¼ ë¬¸ì¥ ì¶”ë¡ ì´ê¸° ë•Œë¬¸ì— íŒ¨ë”©í•  í•„ìš”ê°€ ì—†ìŒ 
  score = mymodel.predict(np.array([input_id])) 

  print("** INPUT:", sentence)
  print("   -> ë¶€ì •: {:.2f} / ê¸ì •: {:.2f}".format(score[0][0],score[0][1]))

sentence1 = "ì‹œê°„ ê°€ëŠ” ì¤„ ì•Œê³  ë´¤ìŠµë‹ˆë‹¤."
sentence2 = "ì•ˆë³´ë©´ í›„íšŒã… ã… ..."
inference(model, sentence1)
inference(model, sentence2)

# ì›í•˜ëŠ” ë¬¸ì¥ì— ëŒ€í•´ ì¶”ë¡ í•´ ë³´ì„¸ìš”
inference(model, "ë°•ì„œì¤€ì´ ë‹¤í–ˆë”°")
inference(model, "ê¿€ì  ì¤ìŠµë‹ˆë‹¤")

"""# # 3. ë‚˜ë§Œì˜ ëª¨ë¸ ë§Œë“¤ì–´ë³´ê¸°

# ì‹¤ìŠµ MISSION #16
>  LSTM, Dense layer ë“±ì„ ììœ ë¡­ê²Œ í™œìš©í•´ì„œ ìì‹ ë§Œì˜ ëª¨ë¸ì„ ë§Œë“¤ê³  
ì´í›„ TEST ë°ì´í„°ì— ëŒ€í•´ ìµœì¢… ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì„¸ìš”
</font>
"""

tf.keras.backend.clear_session()
 
# 1. ëª¨ë¸ êµ¬í˜„í•˜ê¸°
model2 = tf.keras.Sequential([
                              # MISSION ì‘ì„± #
                              tf.keras.layers.Embedding(vocab_size, 150),
                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
                              # tf.keras.layers.Dropout(rate = 0.3),
                              tf.keras.layers.Dense(100, activation='relu'),
                              tf.keras.layers.Dense(2, activation='softmax')
                              ################                 
])

layers.Bidirectional(layers.LSTM(64, return_sequences=True), 
                               input_shape=(5, 10))


# 2. optimizer, loss ì„ íƒí•˜ê¸°
model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# 3. ëª¨ë¸ í›ˆë ¨í•˜ê¸°
num_epochs = 1
history = model2.fit(train_input_ids, train_label_ids, epochs=num_epochs, validation_data=(val_input_ids, val_label_ids), verbose=2)

# 4. ëª¨ë¸ ì§„ë‹¨í•˜ê¸°

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

# 5. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ í‰ê°€í•˜ê¸°

model2.evaluate(test_input_ids, test_label_ids, verbose=2)

# ìƒ˜í”Œ ì˜ˆì œì— ëŒ€í•´ ì¶”ë¡ í•´ ë³´ì„¸ìš” 

inference(model2, "ë¬¼ì´ ë°˜ë„ ì•ˆë‚¨ì•˜ë‹¤")  #ë¶€ì •
inference(model2, "ë¬¼ì´ ë°˜ì´ë‚˜ ë‚¨ì•˜ë‹¤")  #ê¸ì •
inference(model2, "ì£„ì†¡í•˜ì§€ë§Œ í˜¹ì‹œ ì‹¤ë¡€ê°€ ì•ˆëœë‹¤ë©´ êº¼ì ¸ì£¼ì‹¤ìˆ˜ ìˆìœ¼ì‹ ì§€ã…ã…?") #ë¶€ì •
inference(model2, "ì˜í•˜ëŠ” ì§“ì´ë‹¤") #ë¶€ì •
inference(model2, "ê°€ê²Œ ì™¸ê´€ì€ êµ¬ë¦°ë° ë§›ì€ ã…‡ã…ˆ") #ê¸ì •
inference(model2, "ã„·ã„· ê°„ë§Œì— ê°“ëµì‘ ã„·ã„·ã„·") #ê¸ì •
inference(model2, "ì£¼ì¸ê³µ ì»¤ì—¬ì›Œ ã… ã… ") #ê¸ì •
inference(model2, "OTL") #ë¶€ì •

